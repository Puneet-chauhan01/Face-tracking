{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc299625-915c-40c5-875a-2ff2a96ed40f",
   "metadata": {},
   "source": [
    "## Face-tracking\n",
    "This script can track and identify multiple faces and store last 5 centroids of bboxe predictions and also visualizes them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057b31f1-cd03-4937-af93-d85eab311279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_encoding_images(images_path):\n",
    "        # Load Images\n",
    "        images_path = glob.glob(os.path.join(images_path, \"*.*\"))\n",
    "\n",
    "        print(\"{} encoding images found.\".format(len(images_path)))\n",
    "\n",
    "        for img_path in images_path:\n",
    "            img = cv2.imread(img_path)\n",
    "            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            basename = os.path.basename(img_path)\n",
    "            (filename, ext) = os.path.splitext(basename)\n",
    "            # Get encoding\n",
    "            img_encoding = face_recognition.face_encodings(rgb_img)[0]\n",
    "            \n",
    "            known_face_encodings.append(img_encoding)\n",
    "            known_face_names.append(filename)\n",
    "        print(\"Encoding images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2feb8958-0580-4f01-a8cf-ace8139cf4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 encoding images found.\n",
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trackers = []\n",
    "face_names = []\n",
    "known_face_encodings = []  # Pre-loaded known face encodings\n",
    "known_face_names = []  # Corresponding names of known faces\n",
    "load_encoding_images('encoding')\n",
    "ground_truth_values = [(659, 383, 586, 694), (615, 371, 561, 706), (499, 306, 534, 725), (549, 344, 547, 732), (669, 358, 573, 720),\n",
    "     (664, 382, 580, 681), (487, 291, 526, 752), (469, 265, 523, 744), (667, 378, 576, 700), (659, 344, 570, 720),\n",
    "     (647, 382, 580, 701), (480, 279, 526, 750), (673, 377, 577, 700), (659, 344, 570, 720), (636, 379, 571, 698),\n",
    "     (673, 367, 573, 703), (522, 321, 538, 749), (670, 379, 577, 700), (471, 267, 528, 745), (583, 356, 555, 720)]\n",
    "frame_index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50af0a82-f4a1-4bc6-a695-5c4b3f866154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "# for webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"sampl-vedio.mp4\")\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Initialize lists for trackers and face names\n",
    "trackers = []\n",
    "face_names = []\n",
    "\n",
    "# Initialize a list to store centroids of the last 5 frames\n",
    "centroids_history = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB (dlib uses RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    detections = face_detector(rgb_frame)\n",
    "\n",
    "    # Clear previous trackers\n",
    "    trackers = []\n",
    "\n",
    "    # Update centroids history with current detections\n",
    "    current_centroids = []\n",
    "    current_names = []\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2 = int(detection.left()), int(detection.top()), int(detection.right()), int(detection.bottom())\n",
    "\n",
    "        # Calculate the centroid\n",
    "        centroid_x = int((x1 + x2) / 2)\n",
    "        centroid_y = int((y1 + y2) / 2)\n",
    "\n",
    "        # Store current centroid in history\n",
    "        current_centroids.append((centroid_x, centroid_y))\n",
    "\n",
    "        # Face Recognition (you can modify this part as per your recognition logic)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(y1, x2, y2, x1)])\n",
    "\n",
    "        if face_encodings:\n",
    "            face_encoding = face_encodings[0]\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "            else:\n",
    "                known_face_encodings.append(face_encoding)\n",
    "                name = f\"Person {len(known_face_names) + 1}\"\n",
    "                known_face_names.append(name)\n",
    "\n",
    "            current_names.append(name)\n",
    "\n",
    "    # Update centroids history with current centroids\n",
    "    centroids_history.append((current_centroids, current_names))\n",
    "\n",
    "    # Keep only the last 5 frames' centroids in history\n",
    "    if len(centroids_history) > 5:\n",
    "        centroids_history = centroids_history[-5:]\n",
    "\n",
    "    # Display centroids from previous 5 frames (without bounding boxes and names)\n",
    "    for prev_centroids, _ in centroids_history[:-1]:  # exclude the current frame\n",
    "        for (centroid_x, centroid_y) in prev_centroids:\n",
    "            # Draw circle at centroid for past frames\n",
    "            cv2.circle(frame, (centroid_x, centroid_y), 5, (255, 255, 0), -1)\n",
    "\n",
    "    # Display bounding box and name for current frame's centroids\n",
    "    for (centroid_x, centroid_y), name in zip(current_centroids, current_names):\n",
    "        # Calculate the bounding box coordinates based on face dimensions\n",
    "        face_width = abs(x2 - x1)\n",
    "        face_height = abs(y2 - y1)\n",
    "        bbox_x1 = centroid_x - face_width // 2\n",
    "        bbox_y1 = centroid_y - face_height // 2\n",
    "        bbox_x2 = centroid_x + face_width // 2\n",
    "        bbox_y2 = centroid_y + face_height // 2\n",
    "\n",
    "        # Draw bounding box around current centroid\n",
    "        cv2.rectangle(frame, (bbox_x1, bbox_y1), (bbox_x2, bbox_y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Display dynamically fetched name next to current centroid\n",
    "        cv2.putText(frame, f\"My Name: {name}\", (bbox_x1, bbox_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bcafa-1c8b-4043-9d7b-ce8e3d0e24b9",
   "metadata": {},
   "source": [
    "## This script works same as prev but doesnt store or display prev centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833b413a-e473-432a-9576-986b5beaaffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 encoding images found.\n",
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "# Initialize video capture from file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize known face encodings and names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load encoding images and initialize face detector\n",
    "load_encoding_images(\"encoding\")\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Initialize lists for centroids and face names\n",
    "centroids = []\n",
    "face_names = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB (dlib uses RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    detections = face_detector(rgb_frame)\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2 = int(detection.left()), int(detection.top()), int(detection.right()), int(detection.bottom())\n",
    "\n",
    "        # Calculate the centroid\n",
    "        centroid_x = int((x1 + x2) / 2)\n",
    "        centroid_y = int((y1 + y2) / 2)\n",
    "\n",
    "        # Face Recognition\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(y1, x2, y2, x1)])\n",
    "\n",
    "        if face_encodings:\n",
    "            face_encoding = face_encodings[0]\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "            else:\n",
    "                known_face_encodings.append(face_encoding)\n",
    "                name = f\"Person {len(known_face_names) + 1}\"\n",
    "                known_face_names.append(name)\n",
    "\n",
    "            # Draw bounding box around face\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Track centroid with circle\n",
    "            cv2.circle(frame, (centroid_x, centroid_y), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, name, (centroid_x - 20, centroid_y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fcf768-8ada-41c9-afc0-41c536c1469a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcls",
   "language": "python",
   "name": "imgcls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
