





!pip install opencv-python


import cv2
import numpy as np
from ultralytics import YOLO


capture = cv2.VideoCapture(0)
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
while True:
  ret, frame = capture.read()
  faces = face.detectMultiScale(
        cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        )
  for (x, y, w, h) in faces:
    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)

  cv2.imshow('Face-Tracking(Haarcascade)',frame)
  if cv2.waitKey(10) == ord('q'):
    break
capture.release()
cv2.destroyAllWindows()


img = cv2.imread("smpl.jpg")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
faces = face.detectMultiScale(
    cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),
    scaleFactor=1.1,
    minNeighbors=5,
    minSize=(5, 5),
)
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 255), 2)
cv2.imshow('Face-Tracking(Haarcascade)',img)
cv2.waitKey(0)
cv2.destroyAllWindows()


import cv2
import time

# Load video capture and face detector
capture = cv2.VideoCapture("VID_20240616211418.mp4")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Initialize variables
total_time = 0
frame_count = 0

# Placeholder for ground truth face data
# Replace this with actual ground truth data for each frame
ground_truth_faces = [
    [(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
    [(583, 356, 555, 720)]
]
    # [(x1, y1, w1, h1), (x2, y2, w2, h2), ...]
    # Add your actual ground truth face data here


while True:
    ret, frame = capture.read()
    
    if not ret:
        break
    
    start = time.time()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30)
    )
    end = time.time()
    
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1
    
    frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
    
    for detected_face in faces:
        cv2.rectangle(frame, (detected_face[0], detected_face[1]), (detected_face[0]+detected_face[2], detected_face[1]+detected_face[3]), (0, 255, 255), 2)
        
        for ground_truth_face in frame_ground_truth_faces:
            iou = calculate_iou(detected_face, ground_truth_face)
            print(f'Frame {frame_count} - IoU: {iou:.6f}')
    
    cv2.imshow('Face-Tracking(Haarcascade)', frame)
    
    if cv2.waitKey(10) == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

capture.release()
cv2.destroyAllWindows()






!pip install ultralytics



!pip list



from ultralytics import YOLO


model = YOLO("yolov8n.pt")
model3 =YOLO("yolov8m.pt")



img = cv2.imread("smpl.jpg")
result = model3.predict(source = img , classes = [0])
box = []
coords = []
for r in result:
    box = r.boxes
    coords = box.xyxy
    coords = coords.tolist()
for (x1, y1, x2, y2) in coords:
        cv2.rectangle(img, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
# print(coords)
cv2.imshow('Face-Tracking(YOLO)',img)
cv2.waitKey(0)
cv2.destroyAllWindows()


!pip install roboflow


!nvidia-smi


# results = model3.train(data="Face-Detection-1/data.yaml", epochs=10, imgsz=640 )


import torch
torch.cuda.is_available()





!pip list


import ultralytics
from ultralytics import YOLO


new_model = YOLO("best.pt")#downloaded after training from collab


img = cv2.imread("encoding/Puneet.jpg")
img = cv2.resize(img,[640,640])
result = new_model.predict(source = img , classes = [0])
box = []
coords = []
for r in result:
    box = r.boxes
    coords = box.xyxy
    coords = coords.tolist()
for (x1, y1, x2, y2) in coords:
        cv2.rectangle(img, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
# print(coords)
cv2.imshow('Face-Tracking(YOLO)',img)
cvb.waitKey(0)
cv2.destroyAllWindows()





import cv2
import dlib
import face_recognition
import numpy as np

# Initialize video capture from file
cap = cv2.VideoCapture(0)

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load encoding images and initialize face detector
load_encoding_images("encoding")
# Initialize Dlib's face detector (it might still be useful for other purposes)
face_detector = dlib.get_frontal_face_detector()

# Initialize lists for trackers and face names
trackers = []
face_names = []

# Initialize a list to store centroids of the last 5 frames
centroids_history = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert frame to RGB (dlib uses RGB)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Use YOLO model for face detection
    result = new_model.predict(source=frame, classes=[0])

    current_centroids = []
    current_names = []
    coords = []

    for r in result:
        boxes = r.boxes
        if boxes is not None:
            coords = boxes.xyxy.tolist()
    
    for (x1, y1, x2, y2) in coords:
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)

        # Calculate the centroid
        centroid_x = (x1 + x2) // 2
        centroid_y = (y1 + y2) // 2

        # Store current centroid in history
        current_centroids.append((centroid_x, centroid_y))

        # Face Recognition
        face_encodings = face_recognition.face_encodings(rgb_frame, [(y1, x2, y2, x1)])

        if face_encodings:
            face_encoding = face_encodings[0]
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if True in matches:
                first_match_index = matches.index(True)
                name = known_face_names[first_match_index]
            else:
                known_face_encodings.append(face_encoding)
                name = f"Person {len(known_face_names) + 1}"
                known_face_names.append(name)

            current_names.append(name)

    # Update centroids history with current centroids
    centroids_history.append((current_centroids, current_names))

    # Keep only the last 5 frames' centroids in history
    if len(centroids_history) > 5:
        centroids_history = centroids_history[-5:]

    # Display centroids from previous 5 frames (without bounding boxes and names)
    for prev_centroids, _ in centroids_history[:-1]:  # exclude the current frame
        for (centroid_x, centroid_y) in prev_centroids:
            # Draw circle at centroid for past frames
            cv2.circle(frame, (centroid_x, centroid_y), 5, (255, 255, 0), -1)

    # Display bounding box and name for current frame's centroids
    for (centroid_x, centroid_y), name in zip(current_centroids, current_names):
        # Draw bounding box around current centroid
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # Display dynamically fetched name next to current centroid
        cv2.putText(frame, f"My Name: {name}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the frame
    cv2.imshow('Face-Tracking(YOLO)', frame)

    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
cap.release()
cv2.destroyAllWindows()









!pip install face_recognition


!pip install opencv-python dlib face_recognition numpy



import face_recognition





from PIL import Image
import face_recognition

image = face_recognition.load_image_file("smpl.jpg")
face_locations = face_recognition.face_locations(image)

print("found {} face(s) in this photograph.".format(len(face_locations)))

for face_location in face_locations:

    # Print the location of each face in this image
    top, right, bottom, left = face_location
    print("A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}".format(top, left, bottom, right))

    # You can access the actual face itself like this:
    face_image = image[top:bottom, left:right]
    pil_image = Image.fromarray(face_image)
    pil_image.show()











import glob
import cv2
import dlib
import face_recognition
import numpy as np
import os
import glob

def load_encoding_images(images_path):
        # Load Images
        images_path = glob.glob(os.path.join(images_path, "*.*"))

        print("{} encoding images found.".format(len(images_path)))

        for img_path in images_path:
            img = cv2.imread(img_path)
            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            basename = os.path.basename(img_path)
            (filename, ext) = os.path.splitext(basename)
            # Get encoding
            img_encoding = face_recognition.face_encodings(rgb_img)[0]
            
            known_face_encodings.append(img_encoding)
            known_face_names.append(filename)
        print("Encoding images loaded")


import cv2
import dlib
import face_recognition
import numpy as np
import os
import glob
trackers = []
face_names = []
known_face_encodings = []  # Pre-loaded known face encodings
known_face_names = []  # Corresponding names of known faces
load_encoding_images('encoding')
ground_truth_values = [(659, 383, 586, 694), (615, 371, 561, 706), (499, 306, 534, 725), (549, 344, 547, 732), (669, 358, 573, 720),
     (664, 382, 580, 681), (487, 291, 526, 752), (469, 265, 523, 744), (667, 378, 576, 700), (659, 344, 570, 720),
     (647, 382, 580, 701), (480, 279, 526, 750), (673, 377, 577, 700), (659, 344, 570, 720), (636, 379, 571, 698),
     (673, 367, 573, 703), (522, 321, 538, 749), (670, 379, 577, 700), (471, 267, 528, 745), (583, 356, 555, 720)]
frame_index = 0 


import cv2
import dlib
import face_recognition
import numpy as np

# Initialize video capture from file
cap = cv2.VideoCapture(0)

face_detector = dlib.get_frontal_face_detector()

# Initialize lists for trackers and face names
trackers = []
face_names = []

# Initialize a list to store centroids of the last 5 frames
centroids_history = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert frame to RGB (dlib uses RGB)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Detect faces in the frame
    detections = face_detector(rgb_frame)

    # Clear previous trackers
    trackers = []

    # Update centroids history with current detections
    current_centroids = []
    current_names = []
    for detection in detections:
        x1, y1, x2, y2 = int(detection.left()), int(detection.top()), int(detection.right()), int(detection.bottom())

        # Calculate the centroid
        centroid_x = int((x1 + x2) / 2)
        centroid_y = int((y1 + y2) / 2)

        # Store current centroid in history
        current_centroids.append((centroid_x, centroid_y))

        # Face Recognition (you can modify this part as per your recognition logic)
        face_encodings = face_recognition.face_encodings(rgb_frame, [(y1, x2, y2, x1)])

        if face_encodings:
            face_encoding = face_encodings[0]
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if True in matches:
                first_match_index = matches.index(True)
                name = known_face_names[first_match_index]
            else:
                known_face_encodings.append(face_encoding)
                name = f"Person {len(known_face_names) + 1}"
                known_face_names.append(name)

            current_names.append(name)

    # Update centroids history with current centroids
    centroids_history.append((current_centroids, current_names))

    # Keep only the last 5 frames' centroids in history
    if len(centroids_history) > 5:
        centroids_history = centroids_history[-5:]

    # Display centroids from previous 5 frames (without bounding boxes and names)
    for prev_centroids, _ in centroids_history[:-1]:  # exclude the current frame
        for (centroid_x, centroid_y) in prev_centroids:
            # Draw circle at centroid for past frames
            cv2.circle(frame, (centroid_x, centroid_y), 5, (255, 255, 0), -1)

    # Display bounding box and name for current frame's centroids
    for (centroid_x, centroid_y), name in zip(current_centroids, current_names):
        # Calculate the bounding box coordinates based on face dimensions
        face_width = abs(x2 - x1)
        face_height = abs(y2 - y1)
        bbox_x1 = centroid_x - face_width // 2
        bbox_y1 = centroid_y - face_height // 2
        bbox_x2 = centroid_x + face_width // 2
        bbox_y2 = centroid_y + face_height // 2

        # Draw bounding box around current centroid
        cv2.rectangle(frame, (bbox_x1, bbox_y1), (bbox_x2, bbox_y2), (0, 255, 0), 2)

        # Display dynamically fetched name next to current centroid
        cv2.putText(frame, f"My Name: {name}", (bbox_x1, bbox_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the frame
    cv2.imshow('Video', frame)

    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
cap.release()
cv2.destroyAllWindows()






import cv2
import dlib
import face_recognition
import numpy as np

# Initialize video capture from file
cap = cv2.VideoCapture(0)

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load encoding images and initialize face detector
load_encoding_images("encoding")
face_detector = dlib.get_frontal_face_detector()

# Initialize lists for centroids and face names
centroids = []
face_names = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert frame to RGB (dlib uses RGB)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Detect faces in the frame
    detections = face_detector(rgb_frame)

    # Iterate over detected faces
    for detection in detections:
        x1, y1, x2, y2 = int(detection.left()), int(detection.top()), int(detection.right()), int(detection.bottom())

        # Calculate the centroid
        centroid_x = int((x1 + x2) / 2)
        centroid_y = int((y1 + y2) / 2)

        # Face Recognition
        face_encodings = face_recognition.face_encodings(rgb_frame, [(y1, x2, y2, x1)])

        if face_encodings:
            face_encoding = face_encodings[0]
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if True in matches:
                first_match_index = matches.index(True)
                name = known_face_names[first_match_index]
            else:
                known_face_encodings.append(face_encoding)
                name = f"Person {len(known_face_names) + 1}"
                known_face_names.append(name)

            # Draw bounding box around face
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Track centroid with circle
            cv2.circle(frame, (centroid_x, centroid_y), 5, (0, 0, 255), -1)
            cv2.putText(frame, name, (centroid_x - 20, centroid_y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # Display the frame
    cv2.imshow('Video', frame)

    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
cap.release()
cv2.destroyAllWindows()






import cv2
import dlib
import numpy as np
import time

# Initialize face detector and known ground truth face locations
face_detector = dlib.get_frontal_face_detector()
ground_truth_values = [
    [(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
    [(583, 356, 555, 720)]
]

def calculate_iou(boxA, boxB):
    x1A, y1A, wA, hA = boxA
    x2A, y2A = x1A + wA, y1A + hA
    
    x1B, y1B, wB, hB = boxB
    x2B, y2B = x1B + wB, y1B + hB

    xA = max(x1A, x1B)
    yA = max(y1A, y1B)
    xB = min(x2A, x2B)
    yB = min(y2A, y2B)

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = wA * hA
    boxBArea = wB * hB

    iou = interArea / float(boxAArea + boxBArea - interArea)

    return iou

# Load the video file
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")
frame_index = 0
latency_list = []

while True:
    start_time = time.time()  # Start timing for this frame
    ret, frame = video_capture.read()
    if not ret:
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Face detection
    detections = face_detector(rgb_frame)

    # Draw bounding boxes and calculate IoU with ground truth
    if frame_index < len(ground_truth_values):
        ground_truth_boxes = ground_truth_values[frame_index]
        for detection in detections:
            x1 = detection.left()
            y1 = detection.top()
            x2 = detection.right()
            y2 = detection.bottom()
            pred_box = (x1, y1, x2 - x1, y2 - y1)

            # Draw bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Calculate and print IoU for each detected face
            for gt_box in ground_truth_boxes:
                iou = calculate_iou(pred_box, gt_box)
                print(f"Predicted Box: {pred_box}, Ground Truth Box: {gt_box}, IoU: {iou:.4f}")

    # Display frame
    cv2.imshow('Video', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    # Measure and record latency for this frame
    end_time = time.time()
    latency = end_time - start_time
    latency_list.append(latency)
    print(f"Latency for frame {frame_index}: {latency:.4f} seconds")

    frame_index += 1

# Release video capture and close windows
video_capture.release()
cv2.destroyAllWindows()

# Calculate average latency
if len(latency_list) > 0:
    average_latency = sum(latency_list) / len(latency_list)
    print(f"\nAverage latency per frame: {average_latency:.4f} seconds")
else:
    print("No frames processed.")






import cv2
import dlib
import numpy as np
import time

# Initialize face detector and known ground truth face locations
face_detector = dlib.get_frontal_face_detector()
ground_truth_values = [
    [(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
    [(583, 356, 555, 720)]
]

# Function to calculate IoU between two bounding boxes in (x1, y1, w, h) format
def calculate_iou(boxA, boxB):
    x1A, y1A, wA, hA = boxA
    x2A, y2A = x1A + wA, y1A + hA
    
    x1B, y1B, wB, hB = boxB
    x2B, y2B = x1B + wB, y1B + hB

    xA = max(x1A, x1B)
    yA = max(y1A, y1B)
    xB = min(x2A, x2B)
    yB = min(y2A, y2B)

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = wA * hA
    boxBArea = wB * hB

    iou = interArea / float(boxAArea + boxBArea - interArea)

    return iou

# Load the video file
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")
frame_index = 0
latency_list = []

while True:
    start_time = time.time()  # Start timing for this frame
    ret, frame = video_capture.read()
    if not ret:
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Face detection
    detections = face_detector(rgb_frame)

    # Draw bounding boxes and calculate IoU with ground truth
    if frame_index < len(ground_truth_values):
        ground_truth_boxes = ground_truth_values[frame_index]
        for detection in detections:
            x1 = detection.left()
            y1 = detection.top()
            x2 = detection.right()
            y2 = detection.bottom()
            pred_box = (x1, y1, x2 - x1, y2 - y1)

            # Draw bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Calculate and print IoU for each detected face
            for gt_box in ground_truth_boxes:
                iou = calculate_iou(pred_box, gt_box)
                print(f"Predicted Box: {pred_box}, Ground Truth Box: {gt_box}, IoU: {iou:.4f}")

    # Display frame
    cv2.imshow('Video', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    # Measure and record latency for this frame
    end_time = time.time()
    latency = end_time - start_time
    latency_list.append(latency)
    print(f"Latency for frame {frame_index}: {latency:.4f} seconds")

    frame_index += 1

# Release video capture and close windows
video_capture.release()
cv2.destroyAllWindows()

# Calculate average latency
if len(latency_list) > 0:
    average_latency = sum(latency_list) / len(latency_list)
    print(f"\nAverage latency per frame: {average_latency:.4f} seconds")
else:
    print("No frames processed.")






import face_recognition
import cv2
import numpy as np
import time

# Function to calculate Intersection over Union (IoU) between two bounding boxes
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Load known face encodings and names (you need to implement this function)
def load_encoding_images(folder_path):
    # Implement your loading logic here
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")  # Implement this function

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
  (659, 383, 586, 694),    # (top, left, height, width)
    (615, 371, 561, 706),
    (499, 306, 559, 752),
    (549, 344, 527, 732),
    (669, 358, 608, 719),
    (664, 382, 613, 721),
    (487, 291, 530, 752),
    (469, 265, 744, 744),
    (667, 378, 576, 754),
    (659, 344, 570, 747),
    (647, 382, 582, 750),
    (480, 279, 726, 726),
    (673, 377, 576, 749),
    (659, 344, 570, 747),
    (636, 379, 570, 827),
    (673, 367, 571, 867),
    (522, 321, 739, 749),
    (670, 379, 577, 755),
    (471, 267, 728, 745),
    (583, 356, 581, 721)
]

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Resize frame to speed up face detection
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

    # Detect faces in the frame
    face_locations = face_recognition.face_locations(rgb_small_frame)
    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

    # Process each detected face
    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
        # Scale coordinates back to original frame size
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        # Draw rectangle around the detected face (predicted)
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

    # Draw rectangles for ground truth faces
    # Draw rectangles for ground truth faces
    for gt_face in ground_truth_faces:
        if len(gt_face) == 4:  # Ensure each ground truth face annotation has 4 elements (xmin, ymin, width, height)
            xmin, ymin, width, height = gt_face
            cv2.rectangle(frame, (xmin, ymin), (xmin + width, ymin + height), (0, 255, 0), 2)
        else:
            print(f"Invalid ground truth face annotation: {gt_face}")

# Display the resulting frame
    cv2.imshow('Video', frame)


    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time
import glob
import os

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []
ground_truth_faces = [
    [(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
    [(583, 356, 555, 720)]
]
# Load known face encodings and names
# Assuming you have a function load_encoding_images() that loads the encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0


predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face in face_locations:
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            cv2.rectangle(frame, (detected_face[0] * 4, detected_face[1] * 4), ((detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
                gg.append(face_locations)
                print(f'[({face_locations})]')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {detected_face[0] * 4} {detected_face[1] * 4} {(detected_face[0] + detected_face[2]) * 4} {(detected_face[1] + detected_face[3]) * 4}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')
print(gg)

# Write the predicted bounding boxes to a file
with open('predicted_boxes.txt', 'w') as f:
    for box in predicted_boxes:
        f.write(f"{box}\n")

video_capture.release()
cv2.destroyAllWindows()






!pip install inference inference-sdk


!pip install inference-sdk




from inference_sdk import InferenceHTTPClient

CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c"
)

result = CLIENT.infer("Messi1.webp", model_id="face-recognition-gffkh/1")



result





from inference_sdk import InferenceHTTPClient
import cv2
import numpy as np

# Initialize the InferenceHTTPClient
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c"
)

# Initialize video capture from the webcam
video_capture = cv2.VideoCapture(0)  # Use 0 for default webcam

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Perform face recognition inference on the current frame
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = CLIENT.infer(rgb_frame, model_id="face-recognition-gffkh/1")

    # Check if inference result contains predictions
    if 'predictions' in result:
        predictions = result['predictions']
        image_width = result['image']['width']
        image_height = result['image']['height']

        # Draw bounding boxes on the frame
        for prediction in predictions:
            x = int(prediction['x'])
            y = int(prediction['y'])
            width = int(prediction['width'])
            height = int(prediction['height'])
            confidence = prediction['confidence']

            # Convert relative coordinates to absolute coordinates
            left = x
            top = y
            right = x + width
            bottom = y + height

            # Draw bounding box
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

            # Display label with confidence
            label = f"{prediction['class']} {confidence:.2f}"
            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    # Exit loop if 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
video_capture.release()
cv2.destroyAllWindows()






from inference_sdk import InferenceHTTPClient
import cv2
import numpy as np
import os
import time

# Initialize the InferenceHTTPClient with the model_id
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c",
)

# Function to calculate Intersection over Union (IoU)
def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Directory containing images
images_dir = "ground-images"

# List to store latency values
latencies = []

# Ground truth bounding boxes for each image (example data, replace with actual)
ground_truth_faces = [[(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
   [(583, 356, 555, 720)]
                    ]

# Process each image in the directory
for filename in os.listdir(images_dir):
    if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
        # Read the image
        image_path = os.path.join(images_dir, filename)
        image = cv2.imread(image_path)

        # Perform inference on the image
        start_time = time.time()

        # Directly infer the image using CLIENT.infer()
        result = CLIENT.infer(image_path, model_id="face-recognition-gffkh/1")

        end_time = time.time()
        latency = end_time - start_time
        latencies.append(latency)

        # Check if inference result contains predictions
        if 'predictions' in result:
            predictions = result['predictions']

            # Draw bounding boxes on the image and calculate IoU
            for prediction in predictions:
                # Extract prediction bounding box
                pred_box = (
                    int(prediction['x']),
                    int(prediction['y']),
                    int(prediction['width']),
                    int(prediction['height'])
                )

                # Find the corresponding ground truth box if available
                gt_box = None
                for gt_face in ground_truth_faces:
                    # Assuming the model predicts only one face per image
                    if len(gt_face) > 0:
                        gt_box = (
                            int(gt_face[0][0]),  # left
                            int(gt_face[0][1]),  # top
                            int(gt_face[0][0] + gt_face[0][3]),  # right
                            int(gt_face[0][1] + gt_face[0][2])   # bottom
                        )
                        ground_truth_faces.remove(gt_face)
                        break

                if gt_box is not None:
                    # Calculate IoU
                    iou = calculate_iou(pred_box, gt_box)
                    print(f"Image: {filename}, IoU: {iou:.4f}")

                    # Draw bounding box and label on the image
                    left, top, width, height = pred_box
                    right = left + width
                    bottom = top + height
                    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)
                    label = f"{prediction['class']} {prediction['confidence']:.2f}"
                    cv2.putText(image, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Display the image with bounding boxes
            cv2.imshow('Image with Bounding Boxes', image)
            cv2.waitKey(0)

# Calculate average latency
if latencies:
    average_latency = sum(latencies) / len(latencies)
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No images were processed.')




